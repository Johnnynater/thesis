{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD FUNCTION\n",
    "# data = pd.read_csv(r'../datasets/RF-dataset.csv')\n",
    "# for colname in ['name', 'keyword', 'common']:\n",
    "#     data[colname] = data[colname].astype(int)\n",
    "# data['label'] = [1 if x == 'nominal' else 0 for x in data['label']]\n",
    "# #X, y = data.iloc[:, :-1], data['label']\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# X_train, X_test, y_train, y_test = data.iloc[:101, :-1], data.iloc[102:, :-1], data.iloc[:101, -1], data.iloc[102:, -1] \n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    total_entries  unique_entries     ratio     variance  name_nom  name_ord  key_ord  common\n",
      "0           48691           47685  0.979339  3337.245700         1         0        0       0\n",
      "1           48691           11386  0.233842  3992.517869         1         0        0       0\n",
      "2           48691              76  0.001561    17.520780         1         0        0       0\n",
      "3           48691             223  0.004580    55.020775         0         0        0       0\n",
      "4           48691               3  0.000062     0.245778         1         0        0       0\n",
      "5           19158             123  0.006420     2.897103         1         0        0       0\n",
      "6           19158               6  0.000313     1.355629         0         0        0       0\n",
      "7           19158               6  0.000313     1.296207         1         0        0       0\n",
      "8             205             126  0.614634    42.625985         1         0        0       0\n",
      "9            2405              99  0.041164    18.601082         0         0        0       0\n",
      "10            558             384  0.688172   104.797733         1         0        0       0\n",
      "11            558              51  0.091398    18.781289         0         0        0       0\n",
      "12            174              17  0.097701     4.529675         0         0        0       0\n",
      "13            174              48  0.275862     8.313358         1         0        0       0\n",
      "14           6896               4  0.000580     0.375419         0         0        1       0\n",
      "15           6896              17  0.002465     0.959470         0         0        0       0\n",
      "16           6896              17  0.002465     0.816398         0         0        0       0\n",
      "17          18207             164  0.009008    45.460641         1         0        0       0\n",
      "18          18207             651  0.035755   183.302172         0         0        0       0\n",
      "19           5110               5  0.000978     0.695221         1         0        0       0\n",
      "20           5110               4  0.000783     0.587823         0         0        0       0\n",
      "21           1470               3  0.002041     0.556686         0         0        0       0\n",
      "22           1470               6  0.004082     1.010712         0         0        0       0\n",
      "23           1470               9  0.006122     1.589495         1         0        0       0\n",
      "24         322971             158  0.000489    53.040064         1         0        0       0\n",
      "25         322971              15  0.000046     4.146007         1         0        0       0\n",
      "26         322971              13  0.000040     2.569028         0         0        0       0\n",
      "27         322971              21  0.000065     8.279578         1         0        0       0\n",
      "28           8124               2  0.000246     0.316531         0         0        0       0\n",
      "29           8124               6  0.000739     0.795070         0         0        0       0\n",
      "30           8124               4  0.000492     0.707481         0         0        0       0\n",
      "31           8124              10  0.001231     2.152068         1         0        0       0\n",
      "32           8124               2  0.000246     0.202881         0         0        0       0\n",
      "33           8124               9  0.001108     1.999338         0         0        0       0\n",
      "34           8124               2  0.000246     0.378259         0         0        0       0\n",
      "35           8124               2  0.000246     0.202676         0         0        0       0\n",
      "36           8124               2  0.000246     0.183461         0         1        0       0\n",
      "37           8124              12  0.001477     2.315619         1         0        0       0\n",
      "38           8124               2  0.000246     0.194506         0         0        0       0\n",
      "39           8124               5  0.000615     1.244089         0         0        0       0\n",
      "40           8124               4  0.000492     0.702338         0         0        0       0\n",
      "41           8124               4  0.000492     0.702338         0         0        0       0\n",
      "42           8124               9  0.001108     1.783878         1         0        0       0\n",
      "43           8124               9  0.001108     1.783878         1         0        0       0\n",
      "44           8124               1  0.000123     0.000000         1         0        0       0\n",
      "45           8124               4  0.000492     0.609856         1         0        0       0\n",
      "46           8124               3  0.000369     0.352135         0         0        0       0\n",
      "47           8124               5  0.000615     0.799322         1         0        0       0\n",
      "48           8124               9  0.001108     1.628944         1         0        0       0\n",
      "49           8124               6  0.000739     1.626178         0         0        0       0\n",
      "50           8124               7  0.000862     1.234489         0         0        0       0\n",
      "51            800              18  0.022500     5.182618         1         0        0       0\n",
      "52            800              18  0.022500     5.182618         1         0        0       0\n",
      "53             61               8  0.131148     2.181268         1         0        0       0\n",
      "54             61              24  0.393443     2.929508         0         0        0       0\n",
      "55           1968             318  0.161585   102.230694         0         0        0       0\n",
      "56           1968            1878  0.954268   195.057551         0         0        0       0\n",
      "57           1968               7  0.003557     1.863222         0         0        0       0\n",
      "58           1968              37  0.018801     9.882855         1         0        0       0\n",
      "59           1000               5  0.005000     0.286778         1         0        0       0\n",
      "60         127016              43  0.000339    11.074540         1         0        0       0\n",
      "61         127016             423  0.003330   144.337821         0         0        0       0\n",
      "62         127016            1225  0.009644   327.274683         0         0        0       0\n",
      "63         127016             698  0.005495   216.865360         0         0        0       0\n",
      "64            480              14  0.029167     4.026656         1         0        0       0\n",
      "65            480              10  0.020833     0.367264         0         1        0       0\n",
      "66            480               3  0.006250     0.484765         0         0        0       0\n",
      "67            480              12  0.025000     4.197331         0         0        0       0\n",
      "0             200               5  0.025000     1.106709         0         0        0       0\n",
      "1             200               4  0.020000     0.678580         0         0        0       0\n",
      "2             200               4  0.020000     0.678580         0         0        0       0\n",
      "3             200               5  0.025000     1.476931         0         0        0       0\n",
      "4             200               4  0.020000     0.678580         0         0        0       0\n",
      "5             200               4  0.020000     0.678580         0         0        0       0\n",
      "6             200               7  0.035000     1.375801         0         0        1       0\n",
      "7             132               4  0.030303     0.698523         0         1        0       0\n",
      "8             132               4  0.030303     0.351358         0         1        1       0\n",
      "9             132               4  0.030303     0.924570         0         0        0       0\n",
      "10           1728               4  0.002315     1.501360         0         0        1       0\n",
      "11           1728               4  0.002315     1.501360         0         0        1       0\n",
      "12           1728               4  0.002315     0.930266         0         0        1       0\n",
      "13           1728               3  0.001736     0.422129         0         0        1       0\n",
      "14           1728               3  0.001736     0.844817         0         0        0       0\n",
      "15           1728               3  0.001736     0.729142         0         0        1       0\n",
      "16           1728               4  0.002315     1.394581         0         0        1       0\n",
      "17          12960               5  0.000386     0.690801         0         0        1       0\n",
      "18          12960               4  0.000309     0.660942         0         0        0       0\n",
      "19          12960               4  0.000309     0.495033         0         0        1       0\n",
      "20          12960               3  0.000231     0.415985         0         0        1       0\n",
      "21          12960               2  0.000154     0.261135         0         0        0       0\n",
      "22          12960               3  0.000231     0.471974         0         0        1       0\n",
      "23          12960               3  0.000231     0.303682         0         0        0       0\n",
      "24          12960               5  0.000386     0.583021         0         0        1       0\n",
      "25            307               3  0.009772     0.171021         0         0        0       0\n",
      "26            307               3  0.009772     0.171021         0         0        0       0\n",
      "27            307               4  0.013029     0.163287         0         0        0       0\n",
      "28            307               4  0.013029     0.461683         1         0        1       0\n",
      "29            307               3  0.009772     0.363653         0         0        0       0\n",
      "30            307               4  0.013029     0.420644         0         0        1       0\n",
      "31         714992               4  0.000006     0.635998         1         0        0       0\n",
      "32         714992               4  0.000006     0.635998         1         0        0       0\n",
      "33         714992               5  0.000007     0.015795         1         0        1       0\n",
      "34         714992               4  0.000006     0.463611         0         0        0       0\n",
      "35         747137              19  0.000025     2.420329         0         1        0       0\n",
      "36        1048575               5  0.000005     0.718487         0         1        1       0\n",
      "37           9649               4  0.000415     0.684813         0         1        1       0\n",
      "38           9649               4  0.000415     0.062948         0         0        0       0\n",
      "39           9649               5  0.000518     0.026725         0         0        1       0\n",
      "40           9649               3  0.000311     0.093094         1         0        0       0\n",
      "41            426               7  0.016432     0.139794         0         0        0       0\n",
      "42            426               7  0.016432     0.139794         0         0        0       0\n",
      "43            426               5  0.011737     0.293977         0         0        1       0\n",
      "44            426               6  0.014085     0.048623         0         0        1       0\n",
      "45            426               5  0.011737     0.071583         0         0        1       0\n",
      "46            426               5  0.011737     0.015226         0         0        1       0\n",
      "47            426               3  0.007042     0.060397         0         0        1       0\n",
      "48            426               4  0.009390     0.040876         0         0        1       0\n",
      "49            426               7  0.016432     0.217876         0         0        1       0\n",
      "50            426              35  0.082160     0.885751         0         0        0       0\n",
      "51            426               6  0.014085     0.560162         0         0        1       0\n",
      "52            426               6  0.014085     0.560162         0         0        1       0\n",
      "53            426               6  0.014085     0.560162         0         0        1       0\n",
      "54            426              18  0.042254     0.501784         0         0        1       0\n",
      "55            426               4  0.009390     0.062045         0         0        1       0\n",
      "56            426               5  0.011737     0.224477         0         1        1       0\n",
      "57            426               5  0.011737     0.147137         0         0        1       0\n",
      "58            426               5  0.011737     0.147137         0         0        1       0\n",
      "59            426               5  0.011737     0.147137         0         0        1       0\n",
      "60            426               5  0.011737     0.163068         0         0        1       0\n",
      "61            426               6  0.014085     0.215705         1         0        0       0\n",
      "62            426               6  0.014085     0.443547         0         0        1       0\n",
      "63            426               6  0.014085     0.110795         0         0        0       0\n",
      "64            426               4  0.009390     0.066049         0         0        0       0\n",
      "65            426               5  0.011737     0.081902         0         0        0       0\n",
      "66            426               6  0.014085     0.128568         0         0        1       0\n",
      "67            426               7  0.016432     0.078478         0         0        1       0\n",
      "68            426               6  0.014085     0.151639         0         0        1       0\n",
      "69            426               8  0.018779     0.192667         0         0        1       0\n",
      "70            426               4  0.009390     0.092797         0         0        0       0\n",
      "71            426               4  0.009390     0.086587         1         0        0       0\n",
      "72            426               7  0.016432     0.182312         0         0        1       0\n",
      "73            426               5  0.011737     0.118323         1         0        0       0\n",
      "74            426               5  0.011737     0.118323         1         0        0       0\n",
      "75            426               5  0.011737     0.118323         1         0        0       0\n",
      "76            426               5  0.011737     0.093681         0         0        1       0\n",
      "77            323               2  0.006192     0.256610         0         0        0       0\n",
      "78            323               3  0.009288     0.431876         0         0        0       0\n",
      "79            323               2  0.006192     0.025594         1         0        1       0\n",
      "80            323               2  0.006192     0.017076         1         0        0       0\n"
     ]
    }
   ],
   "source": [
    "# Import nominal and ordinal data gathered from src/gbc/heuristics.py\n",
    "data_nom = pd.read_csv(r'../datasets/out_nominal.csv')\n",
    "data_ord = pd.read_csv(r'../datasets/out_ordinal.csv')\n",
    "data = pd.concat([data_nom, data_ord])\n",
    "\n",
    "# Convert False/True to 0/1\n",
    "for colname in ['name_nom','name_ord','key_nom','key_ord','common']:\n",
    "    data[colname] = data[colname].astype(int)\n",
    "\n",
    "# Take the mean of the variance, since there will be issues regarding the dimensionality otherwise\n",
    "data['variance'] = data['variance'].apply(lambda x: np.mean(list(map(float, x.split('[')[1].split(']')[0].split()))))\n",
    "data['label'] = [1 if x == 'nominal' else 0 for x in data['label']]\n",
    "\n",
    "# Drop key_nom, as this feature is not used\n",
    "data = data.drop('key_nom',axis=1)\n",
    "\n",
    "# Split into data + label\n",
    "X, y = data.iloc[:, :-1], data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(X.to_string())\n",
    "# print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Old classifier, not used anymore\n",
    "#clf = RandomForestClassifier(n_estimators=300)\n",
    "\n",
    "# Initialize the GBC and fit the model, then estimate its accuracy + F1-score\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pkl.dump(clf, open('trained_gbc.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980\n",
      "F1: 0.9781021897810219\n"
     ]
    }
   ],
   "source": [
    "# Estimate the model's performance using LOOCV, which might be more reliable given that we have little data\n",
    "loo = LeaveOneOut()\n",
    "y_true, y_pred = list(), list()\n",
    "\n",
    "# Copied from https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/\n",
    "for train_ix, test_ix in loo.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[list(train_ix), :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # fit model\n",
    "    clf = GradientBoostingClassifier(max_depth=2, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # evaluate model\n",
    "    yhat = clf.predict(X_test)\n",
    "    # store\n",
    "    y_true.append(y_test.tolist()[0])\n",
    "    y_pred.append(yhat[0])\n",
    "\n",
    "# Evaluate our findings\n",
    "acc = metrics.accuracy_score(y_true, y_pred)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print(\"F1:\", metrics.f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now with test set = columns from datasets not considered in training data\n",
    "X = pd.concat([data.iloc[15:, :-1], data.iloc[:88, :-1], data.iloc[103:, :-1]])\n",
    "X_eval = pd.concat([data.iloc[:15, :-1], data.iloc[88:103, :-1]])\n",
    "y = pd.concat([data.iloc[15:, -1], data.iloc[:88, -1], data.iloc[103:, -1]])\n",
    "y_eval = pd.concat([data.iloc[:15, -1], data.iloc[88:103, -1]])\n",
    "\n",
    "# Initialize + fit model + evaluate our findings\n",
    "clf = GradientBoostingClassifier(max_depth=2)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X_eval)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_eval, y_pred))\n",
    "print(\"F1:\", metrics.f1_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now doing lr: 0.005\n",
      "Accuracy: 0.745 (0.436)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.960 (0.197)\n",
      "now doing lr: 0.01\n",
      "Accuracy: 0.919 (0.272)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.926 (0.261)\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "now doing lr: 0.05\n",
      "Accuracy: 0.960 (0.197)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "now doing lr: 0.1\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "now doing lr: 0.2\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "now doing lr: 0.3\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.980 (0.140)\n",
      "Accuracy: 0.987 (0.115)\n",
      "Accuracy: 0.987 (0.115)\n",
      "Accuracy: 0.987 (0.115)\n",
      "Accuracy: 0.987 (0.115)\n",
      "Accuracy: 0.987 (0.115)\n",
      "Accuracy: 0.987 (0.115)\n",
      "now doing lr: 0.5\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n",
      "Accuracy: 0.973 (0.162)\n"
     ]
    }
   ],
   "source": [
    "# Same as the LOOCV in the above cells, but more compact.\n",
    "from numpy import mean, std\n",
    "\n",
    "ests = [25, 50, 75, 100, 125, 150, 175, 200]\n",
    "# depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "lrs = [0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "scores_lst = []\n",
    "for j in lrs:\n",
    "    print('now doing lr:', j)\n",
    "    for i in ests:\n",
    "        X, y = data.iloc[:, :-1], data['label']\n",
    "        cv = LeaveOneOut()\n",
    "        clf = GradientBoostingClassifier(random_state=0, learning_rate=j, n_estimators=i)\n",
    "        scores = cross_val_score(clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    \n",
    "# # Save this model\n",
    "# clf.fit(X, y)\n",
    "# pkl.dump(clf, open('trained_gbc.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + F1\n",
    "X, y = data.iloc[:, :-1], data['label']\n",
    "cv = LeaveOneOut()\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "scores = cross_val_predict(clf, X, y, cv=cv, n_jobs=-1)\n",
    "conf_mat = confusion_matrix(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79  2]\n",
      " [ 1 67]]\n",
      "F1: 0.9781021897810219\n",
      "Precision: 0.9710144927536232\n",
      "Recall: 0.9852941176470589\n",
      "ROC_AUC: 0.9803013798111837\n",
      "(array([0.        , 0.02469136, 1.        ]), array([0.        , 0.98529412, 1.        ]), array([2, 1, 0], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x155216c7c08>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjzElEQVR4nO3de3wV1bn/8c9DAPECeCGo5VKwUmu4GCAKSLVQRRERsCCI0Eqx2loBWz22WCsVDj1q1Z5KxVK8FC8UEERLK4o/FEQUlATC1aKUUhMMGhGpKJTb8/tjJjmbkMuGZHZM5vt+vfYrc1kz80x2sp+9Zs2sZe6OiIjEV53qDkBERKqXEoGISMwpEYiIxJwSgYhIzCkRiIjEXN3qDuBINWnSxFu1alXdYYiI1Cg5OTkfu3t6aetqXCJo1aoV2dnZ1R2GiEiNYmb/KmudLg2JiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEXGSJwMweN7OPzGxdGevNzCaZ2SYzW2NmnaKKRUREyhZljWAa0Luc9ZcBbcLXDcAfIoxFRETKENlzBO6+xMxalVOkP/CkB/1gLzezE83sdHcviCKeP7/1Pn/J3RrFrkVEUiLjK4341RVtq3y/1dlG0AzIS5jPD5cdxsxuMLNsM8suLCw8qoP9JXcrGwr+fVTbiojUZjXiyWJ3nwpMBcjKyjrqkXQyTm/ErB92q7K4RERqg+qsEWwFWiTMNw+XiYhIClVnIpgHfC+8e6grsDOq9gERESlbZJeGzGwG0ANoYmb5wK+AegDuPgWYD/QBNgFfAN+PKhYRESlblHcNDa1gvQM3RXV8ERFJjp4sFhGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJuUgTgZn1NrONZrbJzMaWsr6lmS0ys1VmtsbM+kQZj4iIHC6yRGBmacBk4DIgAxhqZhkliv0SeMbdOwJXAw9HFY+IiJQuyhrBecAmd9/s7nuBmUD/EmUcaBRONwY+iDAeEREpRZSJoBmQlzCfHy5LdBcw3MzygfnA6NJ2ZGY3mFm2mWUXFhZGEauISGxVd2PxUGCauzcH+gBPmdlhMbn7VHfPcves9PT0lAcpIlKbRZkItgItEuabh8sSXQc8A+Duy4AGQJMIYxIRkRKiTAQrgDZm1trM6hM0Bs8rUeZ94CIAMzubIBHo2o+ISApFlgjcfT8wClgAvENwd9B6M5tgZv3CYrcC15vZamAGMMLdPaqYRETkcHWj3Lm7zydoBE5cNi5hegPQPcoYRESkfNXdWCwiItVMiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pJOBGZ2XJSBiIhI9agwEZjZ+Wa2Afh7OH+OmWlISRGRWiKZGsH/ApcC2wHcfTVwYZRBiYhI6iR1acjd80osOhBBLCIiUg2S6YY6z8zOB9zM6gE3E4wvICIitUAyNYIfATcRDDy/FcgEfhxhTCIikkLJ1AjOcvdhiQvMrDvwRjQhiYhIKiVTI/h9kstERKQGKrNGYGbdgPOBdDO7JWFVIyAt6sBERCQ1yrs0VB84ISzTMGH5v4FBUQYlIiKpU2YicPfXgNfMbJq7/yuFMYmISAol01j8hZndB7QFGhQtdPdvRxaViIikTDKNxdMJupdoDYwHtgArIoxJRERSKJlEcIq7Pwbsc/fX3H0koNqAiEgtkcyloX3hzwIzuxz4ADg5upBERCSVkkkEE82sMXArwfMDjYCfRBmUiIikToWJwN3/Fk7uBHpC8ZPFIiJSC5T3QFkaMJigj6GX3H2dmfUFfgEcC3RMTYgiIhKl8moEjwEtgLeBSWb2AZAFjHX351MQm4iIpEB5iSAL6ODuB82sAbAN+Jq7b09NaCIikgrl3T66190PArj7HmDzkSYBM+ttZhvNbJOZjS2jzGAz22Bm683sz0eyfxERqbzyagTfMLM14bQBXwvnDXB371DejsM2hslALyAfWGFm89x9Q0KZNsDtQHd332FmTStxLiIichTKSwRnV3Lf5wGb3H0zgJnNBPoDGxLKXA9MdvcdAO7+USWPKSIiR6i8Tucq29FcMyBxrON8oEuJMl8HMLM3CLq2vsvdXyq5IzO7AbgBoGXLlpUMS0REEiU1eH2E6gJtgB7AUOARMzuxZCF3n+ruWe6elZ6entoIRURquSgTwVaC20+LNA+XJcoH5rn7Pnf/J/AuQWIQEZEUSSoRmNmxZnbWEe57BdDGzFqbWX3gamBeiTLPE9QGMLMmBJeKNh/hcUREpBIqTARmdgWQC7wUzmeaWckP9MO4+35gFLAAeAd4xt3Xm9kEM+sXFlsAbDezDcAi4DY9pyAiklrJdDp3F8EdQIsB3D3XzFons3N3nw/ML7FsXMK0A7eELxERqQbJXBra5+47SyzzKIIREZHUS6ZGsN7MrgHSwgfAxgBvRhuWiIikSjI1gtEE4xX/B/gzQXfUP4kwJhERSaFkagTfcPc7gDuiDkZERFIvmRrBA2b2jpn9t5m1izwiERFJqQoTgbv3JBiZrBD4o5mtNbNfRh6ZiIikRFIPlLn7NnefBPyI4JmCceVvISIiNUUyD5SdbWZ3mdlagsHr3yToLkJERGqBZBqLHwdmAZe6+wcRxyMiIilWYSJw926pCERERKpHmYnAzJ5x98HhJaHEJ4mTGqFMRERqhvJqBDeHP/umIhAREakeZTYWu3tBOPljd/9X4gv4cWrCExGRqCVz+2ivUpZdVtWBiIhI9SivjeBGgm/+Z5jZmoRVDYE3og5MRERSo7w2gj8DLwJ3A2MTln/m7p9EGpWIiKRMeYnA3X2Lmd1UcoWZnaxkICJSO1RUI+gL5BDcPmoJ6xw4I8K4REQkRcpMBO7eN/yZ1LCUIiJSMyXT11B3Mzs+nB5uZr81s5bRhyYiIqmQzO2jfwC+MLNzgFuBfwBPRRqViIikTDKJYL+7O9AfeMjdJxPcQioiIrVAMr2PfmZmtwPfBS4wszpAvWjDEhGRVEmmRjCEYOD6ke6+jWAsgvsijUpERFImmaEqtwHTgcZm1hfY4+5PRh6ZiIikRDJ3DQ0G3gauAgYDb5nZoKgDExGR1EimjeAO4Fx3/wjAzNKBhcCcKAMTEZHUSKaNoE5REghtT3I7ERGpAZKpEbxkZguAGeH8EGB+dCGJiEgqJTNm8W1m9h3gm+Giqe7+XLRhiYhIqpQ3HkEb4H7ga8Ba4L/cfWuqAhMRkdQo71r/48DfgIEEPZD+/kh3bma9zWyjmW0ys7HllBtoZm5mWUd6DBERqZzyLg01dPdHwumNZrbySHZsZmnAZIKhLvOBFWY2z903lCjXELgZeOtI9i8iIlWjvETQwMw68n/jEBybOO/uFSWG84BN7r4ZwMxmEvRXtKFEuf8G7gVuO8LYRUSkCpSXCAqA3ybMb0uYd+DbFey7GZCXMJ8PdEksYGadgBbu/oKZlZkIzOwG4AaAli3VA7aISFUqb2CanlEeOOy87rfAiIrKuvtUYCpAVlaWRxmXiEjcRPlg2FagRcJ883BZkYZAO2CxmW0BugLz1GAsIpJaUSaCFUAbM2ttZvWBq4F5RSvdfae7N3H3Vu7eClgO9HP37AhjEhGREiJLBO6+HxgFLADeAZ5x9/VmNsHM+kV1XBEROTIVPllsZgYMA85w9wnheMWnufvbFW3r7vMp0R2Fu48ro2yPpCIWEZEqlUyN4GGgGzA0nP+M4PkAERGpBZLpdK6Lu3cys1UA7r4jvOYvIiK1QDI1gn3hU8IOxeMRHIw0KhERSZlkEsEk4DmgqZn9GlgK/E+kUYmISMok0w31dDPLAS4i6F5igLu/E3lkIiKSEsncNdQS+AL4a+Iyd38/ysBERCQ1kmksfoGgfcCABkBrYCPQNsK4REQkRZK5NNQ+cT7sKO7HkUUkIiIpdcRPFofdT3epsKCIiNQIybQR3JIwWwfoBHwQWUQiIpJSybQRNEyY3k/QZvBsNOGIiEiqlZsIwgfJGrr7f6UoHhERSbEy2wjMrK67HwC6pzAeERFJsfJqBG8TtAfkmtk8YDbwedFKd58bcWwiIpICybQRNAC2E4xRXPQ8gQNKBCIitUB5iaBpeMfQOv4vARTRuMEiIrVEeYkgDTiBQxNAESUCEZFaorxEUODuE1IWiYiIVIvyniwurSYgIiK1THmJ4KKURSEiItWmzETg7p+kMhAREakeR9zpnIiI1C5KBCIiMadEICISc0oEIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCIiMadEICISc5EmAjPrbWYbzWyTmY0tZf0tZrbBzNaY2Stm9tUo4xERkcNFlgjC8Y4nA5cBGcBQM8soUWwVkOXuHYA5wG+iikdEREoXZY3gPGCTu292973ATKB/YgF3X+TuX4Szy4HmEcYjIiKliDIRNAPyEubzw2VluQ54sbQVZnaDmWWbWXZhYWEVhigiIl+KxmIzGw5kAfeVtt7dp7p7lrtnpaenpzY4EZFaLpnB64/WVqBFwnzzcNkhzOxi4A7gW+7+nwjjERGRUkRZI1gBtDGz1mZWH7gamJdYwMw6An8E+rn7RxHGIiIiZYgsEbj7fmAUsAB4B3jG3deb2QQz6xcWuw84AZhtZrlmNq+M3YmISESivDSEu88H5pdYNi5h+uIojy8iIhX7UjQWi4hI9VEiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYq1vdAciX2759+8jPz2fPnj3VHYqIJKFBgwY0b96cevXqJb2NEoGUKz8/n4YNG9KqVSvMrLrDEZFyuDvbt28nPz+f1q1bJ72dLg1Jufbs2cMpp5yiJCBSA5gZp5xyyhHX4JUIpEJKAiI1x9H8vyoRiIjEnBKBfOl9+OGHXHPNNZxxxhl07tyZbt268dxzz1Vqn3fddRf3338/AOPGjWPhwoVHtZ/c3Fzmz59fPD9t2jTS09PJzMykbdu2DBo0iC+++KJSsZZ3vHnz5nHPPfcc9f727dvH2LFjadOmDZ06daJbt268+OKLALRq1YqPP/640jGXjLOwsJAuXbrQsWNHXn/9dfr06cOnn35aqf3/5Cc/YcmSJcXzH3/8MfXq1WPKlCmHlDvhhBMOmZ82bRqjRo0qnn/yySdp164d7du3p2PHjsV/I5UxcuRImjZtSrt27cos4+6MGTOGM888kw4dOrBy5cridU888QRt2rShTZs2PPHEE8XLL774Ynbs2FHp+ECJQL7k3J0BAwZw4YUXsnnzZnJycpg5cyb5+fmHld2/f/9RHWPChAlcfPHFR7VtyQ9mgCFDhpCbm8v69eupX78+s2bNOqp9J3O8fv36MXbs2KPe35133klBQQHr1q1j5cqVPP/883z22WdVEeohEuN85ZVXaN++PatWreKCCy5g/vz5nHjiiUnv68CBA4fMb9++neXLl3PhhRcWL5s9ezZdu3ZlxowZSe/3xRdf5He/+x0vv/wya9euZfny5TRu3Djp7csyYsQIXnrppQqP/d577/Hee+8xdepUbrzxRgA++eQTxo8fz1tvvcXbb7/N+PHjiz/8v/vd7/Lwww9XOj7QXUNyBMb/dT0bPvh3le4z4yuN+NUVbctc/+qrr1K/fn1+9KMfFS/76le/yujRo4HgG93cuXPZtWsXBw4c4IUXXqB///7s2LGDffv2MXHiRPr37w/Ar3/9a5544gmaNm1KixYt6Ny5MxD8o/bt25dBgwaRk5PDLbfcwq5du2jSpAnTpk3j9NNPp0ePHnTp0oVFixbx6aef8thjj9GlSxfGjRvH7t27Wbp0Kbfffvshse/fv5/PP/+ck046CYAtW7YwcuRIPv74Y9LT0/nTn/5Ey5Yty1w+e/Zsxo8fT1paGo0bN2bhwoWHHW/37t1kZ2fz0EMPMWLECBo1akR2djbbtm3jN7/5DYMGDeLgwYOMGjWKV199lRYtWlCvXj1GjhxJnz59eOSRR/jnP//JMcccA8Cpp57K4MGDD3sfBgwYQF5eHnv27OHmm2/mhhtu4MCBA1x33XVkZ2djZowcOZKf/vSnTJo0iSlTplC3bl0yMjKYOXMm06ZNIzs7mx/84Af87Gc/K4572bJlnH322WRnZ9OkSROefvppJk2axN69e+nSpQsPP/wwaWlpnHDCCfzwhz9k4cKFTJ48mW9+85vFsT377LP07t37kHhnzJjBAw88wDXXXEN+fj7Nmzev8G/x7rvv5v777+crX/kKAMcccwzXX399hdtV5MILL2TLli3llvnLX/7C9773PcyMrl278umnn1JQUMDixYvp1asXJ598MgC9evXipZdeYujQofTr148LLriAO+64o9IxqkYgX2rr16+nU6dO5ZZZuXIlc+bM4bXXXqNBgwY899xzrFy5kkWLFnHrrbfi7sU1iaJv1CtWrDhsP/v27WP06NHMmTOHnJwcRo4cecg/2f79+3n77bf53e9+x/jx46lfvz4TJkworgEMGTIEgFmzZpGZmUmzZs345JNPuOKKKwAYPXo01157LWvWrGHYsGGMGTOm3OUTJkxgwYIFrF69mnnz5pV5vEQFBQUsXbqUv/3tb8XfwOfOncuWLVvYsGEDTz31FMuWLQNg06ZNtGzZkkaNGlX4Pjz++OPk5OSQnZ3NpEmT2L59O7m5uWzdupV169axdu1avv/97wNwzz33sGrVKtasWXPYpZnMzMxDzuHYY48tXvfOO+8wa9Ys3njjDXJzc0lLS2P69OkAfP7553Tp0oXVq1cfkgQA3njjjeKkDpCXl0dBQQHnnXcegwcPTrpGtm7dukP2U5bp06eTmZl52GvQoEFJHac0W7dupUWLFsXzzZs3Z+vWrWUuBzjppJP4z3/+w/bt24/6uEVUI5CklffNPVVuuukmli5dSv369Ys/zBO/Mbk7v/jFL1iyZAl16tRh69atfPjhh7z++utceeWVHHfccUBwqaKkjRs3sm7dOnr16gUElyBOP/304vXf+c53AOjcuXO53/CGDBnCQw89hLtz0003cd999zF27FiWLVvG3LlzgaBa/7Of/QygzOXdu3dnxIgRDB48uPjYFRkwYAB16tQhIyODDz/8EIClS5dy1VVXUadOHU477TR69uyZ1L4STZo0qbhdJi8vj/fee4+zzjqLzZs3M3r0aC6//HIuueQSADp06MCwYcMYMGAAAwYMSPoYr7zyCjk5OZx77rkA7N69m6ZNmwKQlpbGwIEDS92uoKCA9PT04vlZs2YV12quvvpqRo4cya233lrmcY/0Lpthw4YxbNiwI9omKk2bNuWDDz7glFNOqdR+Iq0RmFlvM9toZpvM7LALmWZ2jJnNCte/ZWatooxHap62bdse0nA2efJkXnnlFQoLC4uXHX/88cXT06dPp7CwkJycHHJzczn11FOTvqfa3Wnbti25ubnk5uaydu1aXn755eL1RZdP0tLSkmqPMDOuuOKKQxoxj8SUKVOYOHEieXl5dO7cOalvfkUxQnA+5TnzzDN5//33+fe/y7/ct3jxYhYuXMiyZctYvXo1HTt2ZM+ePZx00kmsXr2aHj16MGXKFH7wgx8A8MILL3DTTTexcuVKzj333KTbbtyda6+9tvj3v3HjRu666y4geFo2LS2t1O2OPfbYQ97jGTNmMG3aNFq1akW/fv1Ys2YN7733XnHZvXv3Fpf95JNPaNKkCRD8reXk5FQYZxQ1gmbNmpGXl1c8n5+fT7NmzcpcXmTPnj2H1KqOVmSJwMzSgMnAZUAGMNTMMkoUuw7Y4e5nAv8L3BtVPFIzffvb32bPnj384Q9/KF5W3l04O3fupGnTptSrV49Fixbxr3/9Cwiu0z7//PPs3r2bzz77jL/+9a+HbXvWWWdRWFhYfOlk3759rF+/vtz4GjZsWG7j6tKlS/na174GwPnnn8/MmTOB4MPkggsuKHf5P/7xD7p06cKECRNIT08nLy+vwuOVpnv37jz77LMcPHiQDz/8kMWLFwNw3HHHcd1113HzzTcXfzgWFhYye/bsQ7bfuXMnJ510Escddxx///vfWb58ORDcmXPw4EEGDhzIxIkTWblyJQcPHiQvL4+ePXty7733snPnTnbt2pVUnBdddBFz5szho48+AoIP6aL3rzxnn302mzZtAuDdd99l165dbN26lS1btrBlyxZuv/324kbjb33rWzz99NNAUON45plnimtIt99+O7fddhvbtm0DYO/evTz66KOHHW/YsGHFySrxNWfOnKTOszT9+vXjySefxN2LG6lPP/10Lr30Ul5++WV27NjBjh07ePnll7n00kuBIHFu27aNVq1aHfVxi0RZIzgP2OTum919LzAT6F+iTH+g6H6oOcBFpqeXJIGZ8fzzz/Paa6/RunVrzjvvPK699lruvbf07wzDhg0jOzub9u3b8+STT/KNb3wDgE6dOjFkyBDOOeccLrvssuLLD4nq16/PnDlz+PnPf84555xDZmYmb775Zrnx9ezZkw0bNpCZmVl8LbqojaBDhw6sWrWKO++8E4Df//73/OlPf6JDhw489dRTPPjgg+Uuv+2222jfvj3t2rXj/PPP55xzzin1eBUZOHAgzZs3JyMjg+HDh9OpU6fiu2EmTpxIeno6GRkZtGvXjr59+x7WZtC7d2/279/P2WefzdixY+natSsQXNfu0aMHmZmZDB8+nLvvvpsDBw4wfPjw4tsvx4wZk/QdQRkZGUycOJFLLrmEDh060KtXLwoKCirc7vLLLy9ObjNmzODKK6887PyLEsGDDz7I3LlzyczMpGvXrlx11VXFdxv16dOHUaNGcfHFF9O2bVs6depUYW0pGUOHDqVbt25s3LiR5s2b89hjjwFBja+oDaVPnz6cccYZnHnmmVx//fXFdwOdfPLJ3HnnnZx77rmce+65jBs3rvgyaE5ODl27dqVu3Sq4wu/ukbyAQcCjCfPfBR4qUWYd0Dxh/h9Ak1L2dQOQDWS3bNnSj8Zd89b5XfPWHdW2cbZhw4bqDkGqwGeffebu7h9//LGfccYZXlBQUM0RVa3u3bv7jh07qjuMlBozZowvXLiw1HWl/d8C2V7G53WNaCx296nAVICsrKzyL3yW4cvQ0ClSXfr27cunn37K3r17ufPOOznttNOqO6Qq9cADD/D+++8f0fMINV27du246KKLqmRfUSaCrUCLhPnm4bLSyuSbWV2gMVD5e6FE5BBFl05qqy5dulR3CClXFc84FImyjWAF0MbMWptZfeBqYF6JMvOAa8PpQcCrYRVGvkT0lojUHEfz/xpZInD3/cAoYAHwDvCMu683swlmVnQT92PAKWa2CbgFOPpn5SUSDRo0YPv27UoGIjWAh+MRNGjQ4Ii2s5r2D56VleXZ2dnVHUZsaIQykZqlrBHKzCzH3bNK26ZGNBZL9alXr94RjXQkIjWP+hoSEYk5JQIRkZhTIhARibka11hsZoVAxR2QlK4JUDVDLtUcOud40DnHQ2XO+avunl7aihqXCCrDzLLLajWvrXTO8aBzjoeozlmXhkREYk6JQEQk5uKWCKZWdwDVQOccDzrneIjknGPVRiAiIoeLW41ARERKUCIQEYm5WpkIzKy3mW00s01mdliPpmZ2jJnNCte/ZWatqiHMKpXEOd9iZhvMbI2ZvWJmX62OOKtSReecUG6gmbmZ1fhbDZM5ZzMbHL7X683sz6mOsaol8bfd0swWmdmq8O+7T3XEWVXM7HEz+8jM1pWx3sxsUvj7WGNmnSp90LKGLqupLyCNYMjLM4D6wGogo0SZHwNTwumrgVnVHXcKzrkncFw4fWMczjks1xBYAiwHsqo77hS8z22AVcBJ4XzT6o47Bec8FbgxnM4AtlR33JU85wuBTsC6Mtb3AV4EDOgKvFXZY9bGGsF5wCZ33+zue4GZQP8SZfoDT4TTc4CLzMxSGGNVq/Cc3X2Ru38Rzi4nGDGuJkvmfQb4b+BeoDb0o53MOV8PTHb3HQDu/lGKY6xqyZyzA43C6cbABymMr8q5+xLgk3KK9Aee9MBy4EQzO70yx6yNiaAZkJcwnx8uK7WMBwPo7AROSUl00UjmnBNdR/CNoiar8JzDKnMLd38hlYFFKJn3+evA183sDTNbbma9UxZdNJI557uA4WaWD8wHRqcmtGpzpP/vFdJ4BDFjZsOBLOBb1R1LlMysDvBbYEQ1h5JqdQkuD/UgqPUtMbP27v5pdQYVsaHANHd/wMy6AU+ZWTt3P1jdgdUUtbFGsBVokTDfPFxWahkzq0tQndyekuiikcw5Y2YXA3cA/dz9PymKLSoVnXNDoB2w2My2EFxLnVfDG4yTeZ/zgXnuvs/d/wm8S5AYaqpkzvk64BkAd18GNCDonK22Sur//UjUxkSwAmhjZq3NrD5BY/C8EmXmAdeG04OAVz1shamhKjxnM+sI/JEgCdT068ZQwTm7+053b+Lurdy9FUG7SD93r8njnCbzt/08QW0AM2tCcKlocwpjrGrJnPP7wEUAZnY2QSIoTGmUqTUP+F5491BXYKe7F1Rmh7Xu0pC77zezUcACgjsOHnf39WY2Ach293nAYwTVx00EjTJXV1/ElZfkOd8HnADMDtvF33f3ftUWdCUlec61SpLnvAC4xMw2AAeA29y9xtZ2kzznW4FHzOynBA3HI2ryFzszm0GQzJuE7R6/AuoBuPsUgnaQPsAm4Avg+5U+Zg3+fYmISBWojZeGRETkCCgRiIjEnBKBiEjMKRGIiMScEoGISMwpEciXkpkdMLPchFercsruqoLjTTOzf4bHWhk+oXqk+3jUzDLC6V+UWPdmZWMM91P0e1lnZn81sxMrKJ9Z03vjlOjp9lH5UjKzXe5+QlWXLWcf04C/ufscM7sEuN/dO1Rif5WOqaL9mtkTwLvu/utyyo8g6HV1VFXHIrWHagRSI5jZCeE4CivNbK2ZHdbTqJmdbmZLEr4xXxAuv8TMloXbzjazij6glwBnhtveEu5rnZn9JFx2vJm9YGarw+VDwuWLzSzLzO4Bjg3jmB6u2xX+nGlmlyfEPM3MBplZmpndZ2Yrwj7mf5jEr2UZYWdjZnZeeI6rzOxNMzsrfBJ3AjAkjGVIGPvjZvZ2WLa0Hlslbqq772299CrtRfBUbG74eo7gKfhG4bomBE9VFtVod4U/bwXuCKfTCPobakLwwX58uPznwLhSjjcNGBROXwW8BXQG1gLHEzyVvR7oCAwEHknYtnH4czHhmAdFMSWUKYrxSuCJcLo+QS+SxwI3AL8Mlx8DZAOtS4lzV8L5zQZ6h/ONgLrh9MXAs+H0COChhO3/BxgeTp9I0BfR8dX9futVva9a18WE1Bq73T2zaMbM6gH/Y2YXAgcJvgmfCmxL2GYF8HhY9nl3zzWzbxEMVvJG2LVGfYJv0qW5z8x+SdBPzXUE/dc85+6fhzHMBS4AXgIeMLN7CS4nvX4E5/Ui8KCZHQP0Bpa4++7wclQHMxsUlmtM0FncP0tsf6yZ5Ybn/w7w/xLKP2FmbQi6WahXxvEvAfqZ2X+F8w2AluG+JKaUCKSmGAakA53dfZ8FPYo2SCzg7kvCRHE5MM3MfgvsAP6fuw9N4hi3ufucohkzu6i0Qu7+rgVjHfQBJprZK+4+IZmTcPc9ZrYYuBQYQjDQCgSjTY129wUV7GK3u2ea2XEE/e/cBEwiGIBnkbtfGTasLy5jewMGuvvGZOKVeFAbgdQUjYGPwiTQEzhszGULxmH+0N0fAR4lGO5vOdDdzIqu+R9vZl9P8pivAwPM7DgzO57gss7rZvYV4At3f5qgM7/SxozdF9ZMSjOLoKOwotoFBB/qNxZtY2ZfD49ZKg9GmxsD3Gr/15V6UVfEIxKKfkZwiazIAmC0hdUjC3qllZhTIpCaYjqQZWZrge8Bfy+lTA9gtZmtIvi2/aC7FxJ8MM4wszUEl4W+kcwB3X0lQdvB2wRtBo+6+yqgPfB2eInmV8DEUjafCqwpaiwu4WWCgYEWejD8IgSJawOw0oJBy/9IBTX2MJY1BAOz/Aa4Ozz3xO0WARlFjcUENYd6YWzrw3mJOd0+KiISc6oRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjE3P8HZNngxfr5rIwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.fit(X, y)\n",
    "print(conf_mat)\n",
    "print(\"F1:\", metrics.f1_score(y, y_pred))\n",
    "print('Precision:', metrics.precision_score(y, y_pred))\n",
    "print('Recall:', metrics.recall_score(y, y_pred))\n",
    "print('ROC_AUC:', metrics.roc_auc_score(y, y_pred))\n",
    "print(metrics.roc_curve(y, y_pred))\n",
    "metrics.plot_roc_curve(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
